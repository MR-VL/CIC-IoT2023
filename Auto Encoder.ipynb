{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cdad0b2-ca4a-4bd9-8fbf-620f595ba0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.7.0-cp312-cp312-win_amd64.whl.metadata (29 kB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.22.0-cp312-cp312-win_amd64.whl.metadata (6.3 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.7.0-cp312-cp312-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\vlad.serban\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\vlad.serban\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in c:\\users\\vlad.serban\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\vlad.serban\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\vlad.serban\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\vlad.serban\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\vlad.serban\\appdata\\local\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\vlad.serban\\appdata\\local\\anaconda3\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\vlad.serban\\appdata\\local\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\vlad.serban\\appdata\\local\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Downloading torch-2.7.0-cp312-cp312-win_amd64.whl (212.5 MB)\n",
      "   ---------------------------------------- 0.0/212.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 7.9/212.5 MB 44.2 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 26.7/212.5 MB 70.6 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 44.8/212.5 MB 77.1 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 64.0/212.5 MB 79.9 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 82.6/212.5 MB 82.3 MB/s eta 0:00:02\n",
      "   ------------------ -------------------- 102.2/212.5 MB 84.8 MB/s eta 0:00:02\n",
      "   ---------------------- ---------------- 122.4/212.5 MB 85.0 MB/s eta 0:00:02\n",
      "   ------------------------- ------------- 141.6/212.5 MB 86.1 MB/s eta 0:00:01\n",
      "   ----------------------------- --------- 160.7/212.5 MB 87.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 181.4/212.5 MB 88.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 202.1/212.5 MB 89.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  212.3/212.5 MB 89.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  212.3/212.5 MB 89.3 MB/s eta 0:00:01\n",
      "   --------------------------------------- 212.5/212.5 MB 79.0 MB/s eta 0:00:00\n",
      "Downloading torchvision-0.22.0-cp312-cp312-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 97.0 MB/s eta 0:00:00\n",
      "Downloading torchaudio-2.7.0-cp312-cp312-win_amd64.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.5/2.5 MB 72.0 MB/s eta 0:00:00\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 6.3/6.3 MB 95.8 MB/s eta 0:00:00\n",
      "Installing collected packages: sympy, torch, torchvision, torchaudio\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.2\n",
      "    Uninstalling sympy-1.13.2:\n",
      "      Successfully uninstalled sympy-1.13.2\n",
      "Successfully installed sympy-1.14.0 torch-2.7.0 torchaudio-2.7.0 torchvision-0.22.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "933f8e0d-888c-43df-9907-bb1d3a845bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 77157596.20, Avg Loss per sample: 7.8664\n",
      "Epoch 2, Train Loss: 76032857.99, Avg Loss per sample: 7.7518\n",
      "Epoch 3, Train Loss: 75914793.21, Avg Loss per sample: 7.7397\n",
      "Epoch 4, Train Loss: 75874800.26, Avg Loss per sample: 7.7356\n",
      "Epoch 5, Train Loss: 75853259.27, Avg Loss per sample: 7.7335\n",
      "Epoch 6, Train Loss: 75836671.05, Avg Loss per sample: 7.7318\n",
      "Epoch 7, Train Loss: 75803409.09, Avg Loss per sample: 7.7284\n",
      "Epoch 8, Train Loss: 75790715.54, Avg Loss per sample: 7.7271\n",
      "Epoch 9, Train Loss: 75788672.49, Avg Loss per sample: 7.7269\n",
      "Epoch 10, Train Loss: 75784302.85, Avg Loss per sample: 7.7264\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "# Load your saved data\n",
    "df = pd.read_csv('smote_tomek_data.csv')  \n",
    "target_col = 'label' \n",
    "\n",
    "# Drop label column for unsupervised VAE\n",
    "X = df.drop(columns=[target_col])\n",
    "\n",
    "# Normalize features \n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_val = train_test_split(X_scaled, test_size=0.2, random_state=42)\n",
    "train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(train_tensor), batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(val_tensor), batch_size=256)\n",
    "\n",
    "# Define VAE\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=10):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(64, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(64, latent_dim)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decoder(z), mu, logvar\n",
    "\n",
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vae = VAE(input_dim=X_scaled.shape[1]).to(device)\n",
    "optimizer = optim.Adam(vae.parameters(), lr=1e-3)\n",
    "epochs = 10\n",
    "\n",
    "# Train loop\n",
    "for epoch in range(epochs):\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        data = batch[0].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon, mu, logvar = vae(data)\n",
    "        loss = vae_loss(recon, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = train_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.2f}, Avg Loss per sample: {avg_loss:.4f}\")\n",
    "\n",
    "    \n",
    "torch.save(vae.state_dict(), \"smote_tomek_vae_from_csv.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7bfaa668-b023-479a-b19c-944a48886dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 76773331.23, Avg Loss per sample: 7.8868\n",
      "Epoch 2, Train Loss: 75709175.79, Avg Loss per sample: 7.7775\n",
      "Epoch 3, Train Loss: 75627797.80, Avg Loss per sample: 7.7692\n",
      "Epoch 4, Train Loss: 75586339.46, Avg Loss per sample: 7.7649\n",
      "Epoch 5, Train Loss: 75551604.83, Avg Loss per sample: 7.7613\n",
      "Epoch 6, Train Loss: 75542041.56, Avg Loss per sample: 7.7603\n",
      "Epoch 7, Train Loss: 75523536.76, Avg Loss per sample: 7.7584\n",
      "Epoch 8, Train Loss: 75507089.52, Avg Loss per sample: 7.7568\n",
      "Epoch 9, Train Loss: 75499620.06, Avg Loss per sample: 7.7560\n",
      "Epoch 10, Train Loss: 75489985.92, Avg Loss per sample: 7.7550\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "# Load your saved data\n",
    "df = pd.read_csv('smote_enn_data.csv')  \n",
    "target_col = 'label' \n",
    "\n",
    "# Drop label column for unsupervised VAE\n",
    "X = df.drop(columns=[target_col])\n",
    "\n",
    "# Normalize features \n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_val = train_test_split(X_scaled, test_size=0.2, random_state=42)\n",
    "train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(train_tensor), batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(val_tensor), batch_size=256)\n",
    "\n",
    "# Define VAE\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=10):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(64, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(64, latent_dim)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decoder(z), mu, logvar\n",
    "\n",
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vae = VAE(input_dim=X_scaled.shape[1]).to(device)\n",
    "optimizer = optim.Adam(vae.parameters(), lr=1e-3)\n",
    "epochs = 10\n",
    "\n",
    "# Train loop\n",
    "for epoch in range(epochs):\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        data = batch[0].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon, mu, logvar = vae(data)\n",
    "        loss = vae_loss(recon, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = train_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.2f}, Avg Loss per sample: {avg_loss:.4f}\")\n",
    "\n",
    "    \n",
    "torch.save(vae.state_dict(), \"smote_enn_vae_from_csv.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "44a668bc-280c-48ed-9f47-2c78ac022fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 74954586.96, Avg Loss per sample: 12.9850\n",
      "Epoch 2, Train Loss: 74389779.37, Avg Loss per sample: 12.8871\n",
      "Epoch 3, Train Loss: 74360263.74, Avg Loss per sample: 12.8820\n",
      "Epoch 4, Train Loss: 74337897.49, Avg Loss per sample: 12.8781\n",
      "Epoch 5, Train Loss: 74335071.33, Avg Loss per sample: 12.8777\n",
      "Epoch 6, Train Loss: 74325532.29, Avg Loss per sample: 12.8760\n",
      "Epoch 7, Train Loss: 74325322.45, Avg Loss per sample: 12.8760\n",
      "Epoch 8, Train Loss: 74315980.35, Avg Loss per sample: 12.8743\n",
      "Epoch 9, Train Loss: 74311597.01, Avg Loss per sample: 12.8736\n",
      "Epoch 10, Train Loss: 74306734.12, Avg Loss per sample: 12.8727\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "# Load your saved data\n",
    "df = pd.read_csv('smote_data.csv')  \n",
    "target_col = 'label' \n",
    "\n",
    "# Drop label column for unsupervised VAE\n",
    "X = df.drop(columns=[target_col])\n",
    "\n",
    "# Normalize features \n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_val = train_test_split(X_scaled, test_size=0.2, random_state=42)\n",
    "train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(train_tensor), batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(val_tensor), batch_size=256)\n",
    "\n",
    "# Define VAE\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=10):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(64, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(64, latent_dim)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decoder(z), mu, logvar\n",
    "\n",
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vae = VAE(input_dim=X_scaled.shape[1]).to(device)\n",
    "optimizer = optim.Adam(vae.parameters(), lr=1e-3)\n",
    "epochs = 10\n",
    "\n",
    "# Train loop\n",
    "for epoch in range(epochs):\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        data = batch[0].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon, mu, logvar = vae(data)\n",
    "        loss = vae_loss(recon, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = train_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.2f}, Avg Loss per sample: {avg_loss:.4f}\")\n",
    "\n",
    "    \n",
    "torch.save(vae.state_dict(), \"smote_vae_from_csv.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d90abbf-6a70-4bcc-b6df-c0ca4b729127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 75148178.96, Avg Loss per sample: 7.6582\n",
      "Epoch 2, Train Loss: 74330879.91, Avg Loss per sample: 7.5749\n",
      "Epoch 3, Train Loss: 74275958.13, Avg Loss per sample: 7.5693\n",
      "Epoch 4, Train Loss: 74231801.90, Avg Loss per sample: 7.5648\n",
      "Epoch 5, Train Loss: 74204151.89, Avg Loss per sample: 7.5620\n",
      "Epoch 6, Train Loss: 74195444.64, Avg Loss per sample: 7.5611\n",
      "Epoch 7, Train Loss: 74186570.59, Avg Loss per sample: 7.5602\n",
      "Epoch 8, Train Loss: 74155659.03, Avg Loss per sample: 7.5571\n",
      "Epoch 9, Train Loss: 74142781.41, Avg Loss per sample: 7.5558\n",
      "Epoch 10, Train Loss: 74131805.25, Avg Loss per sample: 7.5546\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "# Load your saved data\n",
    "df = pd.read_csv('adasyn_data.csv')  \n",
    "target_col = 'label' \n",
    "\n",
    "# Drop label column for unsupervised VAE\n",
    "X = df.drop(columns=[target_col])\n",
    "\n",
    "# Normalize features \n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_val = train_test_split(X_scaled, test_size=0.2, random_state=42)\n",
    "train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(train_tensor), batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(val_tensor), batch_size=256)\n",
    "\n",
    "# Define VAE\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=10):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(64, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(64, latent_dim)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decoder(z), mu, logvar\n",
    "\n",
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vae = VAE(input_dim=X_scaled.shape[1]).to(device)\n",
    "optimizer = optim.Adam(vae.parameters(), lr=1e-3)\n",
    "epochs = 10\n",
    "\n",
    "# Train loop\n",
    "for epoch in range(epochs):\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        data = batch[0].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon, mu, logvar = vae(data)\n",
    "        loss = vae_loss(recon, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = train_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.2f}, Avg Loss per sample: {avg_loss:.4f}\")\n",
    "\n",
    "    \n",
    "torch.save(vae.state_dict(), \"adasyn_vae_from_csv.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "415b1869-5a4e-45db-8cba-841f773cf977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 500103.05, Avg Loss per sample: 12.7874\n",
      "Epoch 2, Train Loss: 392542.67, Avg Loss per sample: 10.0371\n",
      "Epoch 3, Train Loss: 379823.10, Avg Loss per sample: 9.7119\n",
      "Epoch 4, Train Loss: 362054.72, Avg Loss per sample: 9.2576\n",
      "Epoch 5, Train Loss: 353372.99, Avg Loss per sample: 9.0356\n",
      "Epoch 6, Train Loss: 349279.16, Avg Loss per sample: 8.9309\n",
      "Epoch 7, Train Loss: 346387.58, Avg Loss per sample: 8.8570\n",
      "Epoch 8, Train Loss: 344754.23, Avg Loss per sample: 8.8152\n",
      "Epoch 9, Train Loss: 343301.06, Avg Loss per sample: 8.7781\n",
      "Epoch 10, Train Loss: 341153.73, Avg Loss per sample: 8.7232\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "# Load your saved data\n",
    "df = pd.read_csv('undersampled_data.csv')  \n",
    "target_col = 'label' \n",
    "\n",
    "# Drop label column for unsupervised VAE\n",
    "X = df.drop(columns=[target_col])\n",
    "\n",
    "# Normalize features \n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_val = train_test_split(X_scaled, test_size=0.2, random_state=42)\n",
    "train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(train_tensor), batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(val_tensor), batch_size=256)\n",
    "\n",
    "# Define VAE\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=10):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(64, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(64, latent_dim)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decoder(z), mu, logvar\n",
    "\n",
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vae = VAE(input_dim=X_scaled.shape[1]).to(device)\n",
    "optimizer = optim.Adam(vae.parameters(), lr=1e-3)\n",
    "epochs = 10\n",
    "\n",
    "# Train loop\n",
    "for epoch in range(epochs):\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        data = batch[0].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon, mu, logvar = vae(data)\n",
    "        loss = vae_loss(recon, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = train_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.2f}, Avg Loss per sample: {avg_loss:.4f}\")\n",
    "\n",
    "    \n",
    "torch.save(vae.state_dict(), \"undersampled_vae_from_csv.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c06e9c1b-bc97-41cc-9d41-dabd9f885de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 2862624.99, Avg Loss per sample: 7.6645\n",
      "Epoch 2, Train Loss: 2501205.84, Avg Loss per sample: 6.6968\n",
      "Epoch 3, Train Loss: 2471769.02, Avg Loss per sample: 6.6180\n",
      "Epoch 4, Train Loss: 2458277.50, Avg Loss per sample: 6.5819\n",
      "Epoch 5, Train Loss: 2447872.99, Avg Loss per sample: 6.5540\n",
      "Epoch 6, Train Loss: 2440110.56, Avg Loss per sample: 6.5333\n",
      "Epoch 7, Train Loss: 2436058.80, Avg Loss per sample: 6.5224\n",
      "Epoch 8, Train Loss: 2432953.22, Avg Loss per sample: 6.5141\n",
      "Epoch 9, Train Loss: 2430430.56, Avg Loss per sample: 6.5073\n",
      "Epoch 10, Train Loss: 2427941.69, Avg Loss per sample: 6.5007\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "# Load your saved data\n",
    "df = pd.read_csv('stratified_sample.csv')  \n",
    "target_col = 'label' \n",
    "\n",
    "# Drop label column for unsupervised VAE\n",
    "X = df.drop(columns=[target_col])\n",
    "\n",
    "# Normalize features \n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_val = train_test_split(X_scaled, test_size=0.2, random_state=42)\n",
    "train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(train_tensor), batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(val_tensor), batch_size=256)\n",
    "\n",
    "# Define VAE\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=10):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(64, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(64, latent_dim)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decoder(z), mu, logvar\n",
    "\n",
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vae = VAE(input_dim=X_scaled.shape[1]).to(device)\n",
    "optimizer = optim.Adam(vae.parameters(), lr=1e-3)\n",
    "epochs = 10\n",
    "\n",
    "# Train loop\n",
    "for epoch in range(epochs):\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        data = batch[0].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon, mu, logvar = vae(data)\n",
    "        loss = vae_loss(recon, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = train_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.2f}, Avg Loss per sample: {avg_loss:.4f}\")\n",
    "\n",
    "    \n",
    "torch.save(vae.state_dict(), \"stratified_vae_from_csv.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b6aa1210-11dd-46cb-9aec-28a09ff1f839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 135946076.37, Avg Loss per sample: 13.8487\n",
      "Epoch 2, Train Loss: 135520314.70, Avg Loss per sample: 13.8053\n",
      "Epoch 3, Train Loss: 135503332.21, Avg Loss per sample: 13.8036\n",
      "Epoch 4, Train Loss: 135495119.22, Avg Loss per sample: 13.8028\n",
      "Epoch 5, Train Loss: 135486037.11, Avg Loss per sample: 13.8019\n",
      "Epoch 6, Train Loss: 135483268.15, Avg Loss per sample: 13.8016\n",
      "Epoch 7, Train Loss: 135477714.80, Avg Loss per sample: 13.8010\n",
      "Epoch 8, Train Loss: 135470131.40, Avg Loss per sample: 13.8002\n",
      "Epoch 9, Train Loss: 135471306.98, Avg Loss per sample: 13.8004\n",
      "Epoch 10, Train Loss: 135471585.90, Avg Loss per sample: 13.8004\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "# Load your saved data\n",
    "df = pd.read_csv('data_diffusion_data.csv')  \n",
    "target_col = 'label' \n",
    "\n",
    "# Drop label column for unsupervised VAE\n",
    "X = df.drop(columns=[target_col])\n",
    "\n",
    "# Normalize features \n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_val = train_test_split(X_scaled, test_size=0.2, random_state=42)\n",
    "train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(train_tensor), batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(val_tensor), batch_size=256)\n",
    "\n",
    "# Define VAE\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=10):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(64, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(64, latent_dim)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decoder(z), mu, logvar\n",
    "\n",
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vae = VAE(input_dim=X_scaled.shape[1]).to(device)\n",
    "optimizer = optim.Adam(vae.parameters(), lr=1e-3)\n",
    "epochs = 10\n",
    "\n",
    "# Train loop\n",
    "for epoch in range(epochs):\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        data = batch[0].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon, mu, logvar = vae(data)\n",
    "        loss = vae_loss(recon, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = train_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.2f}, Avg Loss per sample: {avg_loss:.4f}\")\n",
    "\n",
    "    \n",
    "torch.save(vae.state_dict(), \"data_diffusion_vae_from_csv.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055c84e3-b2f5-4c10-9c1b-dfb2a896d34a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
